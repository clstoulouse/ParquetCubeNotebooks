{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, List, Optional\n",
    "#\n",
    "import enum\n",
    "import functools\n",
    "import operator\n",
    "import shutil\n",
    "import os\n",
    "import tempfile\n",
    "#\n",
    "import dask_jobqueue\n",
    "import dask.distributed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = dask_jobqueue.PBSCluster(cores=1,\n",
    "                                   memory='4GB',\n",
    "                                   interface='ib0',\n",
    "                                   local_directory=\"$TMPDIR\",\n",
    "                                   walltime='12:00:00')\n",
    "cluster.scale(10)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = dask.distributed.Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root directory containing the datasets to be processed.\n",
    "ROOT = \"/home/ad/briolf/odatis/briolf/bigdata4science\"\n",
    "\n",
    "# File describing the queries to be executed .\n",
    "REQUEST = \"/home/ad/briolf/notebooks/bigdata4bigscience/requests.csv\"\n",
    "\n",
    "# File containing the results of the benchmarks\n",
    "RESULT = \"/home/ad/briolf/notebooks/bigdata4bigscience/results.csv\"\n",
    "\n",
    "# Temporary directory\n",
    "TMPDIR = \"/work/ALT/odatis/briolf/tmp\"\n",
    "\n",
    "# Temporary Zarr store\n",
    "TMPZARR = os.path.join(TMPDIR, \"zarr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Writers(enum.Enum):\n",
    "    \"\"\"List of known writers\"\"\"\n",
    "    NETCDF =1\n",
    "    ZARR = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_path(name: str) -> str:\n",
    "    \"\"\"Gets the dataset path\"\"\"\n",
    "    return os.path.join(ROOT, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_request(path: str) -> pd.DataFrame:\n",
    "    \"\"\"Loading the file describing the queries\"\"\"\n",
    "    result = pd.read_csv(path,\n",
    "                         sep=\";\",\n",
    "                         dtype={\n",
    "                             \"Dataset\": \"str\",\n",
    "                             \"variables\": \"str\",\n",
    "                             \"minLongitude\": \"float64\",\n",
    "                             \"maxLongitude\": \"float64\",\n",
    "                             \"minLatitude\": \"float64\",\n",
    "                             \"maxLatitude\": \"float64\",\n",
    "                             \"minTime(epoch ms)\": \"str\",\n",
    "                             \"maxTime(epoch(ms)\": \"str\",\n",
    "                             \"minDate\": \"str\",\n",
    "                             \"maxDate\": \"str\",\n",
    "                             \"depth\": \"float64\",\n",
    "                             \"Cores\": \"int\",\n",
    "                             \"mem\": \"str\"\n",
    "                         })\n",
    "    result.drop(\"minTime(epoch ms)\", axis=1, inplace=True)\n",
    "    result.drop(\"maxTime(epoch(ms)\", axis=1, inplace=True)\n",
    "    result = result.assign(minDate=result[\"minDate\"].astype(\"datetime64\"),\n",
    "                           maxDate=result[\"maxDate\"].astype(\"datetime64\"),\n",
    "                           average=float(\"nan\"),\n",
    "                           stdev=float(\"nan\"),\n",
    "                           best=float(\"nan\"),\n",
    "                           worst=float(\"nan\"),\n",
    "                           loops=float(\"nan\"),\n",
    "                           repeat=float(\"nan\"),\n",
    "                           netcdf=float(\"nan\"),\n",
    "                           zarr=float(\"nan\"),\n",
    "                           nbytes=0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_result(path: str) -> pd.DataFrame:\n",
    "    \"\"\"Loading the file containing the intermediate results.\"\"\"\n",
    "    result = pd.read_csv(path,\n",
    "                         sep=\";\",\n",
    "                         dtype={\n",
    "                             \"Dataset\": \"str\",\n",
    "                             \"variables\": \"str\",\n",
    "                             \"minLongitude\": \"float64\",\n",
    "                             \"maxLongitude\": \"float64\",\n",
    "                             \"minLatitude\": \"float64\",\n",
    "                             \"maxLatitude\": \"float64\",\n",
    "                             \"minDate\": \"str\",\n",
    "                             \"maxDate\": \"str\",\n",
    "                             \"depth\": \"float64\",\n",
    "                             \"Cores\": \"int\",\n",
    "                             \"mem\": \"str\",\n",
    "                             \"average\": \"float64\",\n",
    "                             \"stdev\": \"float64\",\n",
    "                             \"best\": \"float64\",\n",
    "                             \"worst\": \"float64\",\n",
    "                             \"loops\": \"float64\",\n",
    "                             \"repeat\": \"float64\",\n",
    "                             \"netcdf\": \"float64\",\n",
    "                             \"zarr\": \"float64\",\n",
    "                             \"nbytes\": \"uint64\",\n",
    "                         })\n",
    "    result = result.assign(minDate=result[\"minDate\"].astype(\"datetime64\"),\n",
    "                            maxDate=result[\"maxDate\"].astype(\"datetime64\"))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def varname_from_standard_name(ds: xr.Dataset,\n",
    "                               standard_names: Iterable[str]) -> List[str]:\n",
    "    \"\"\"Get variable names from standard names.\"\"\"\n",
    "    result = []\n",
    "    for name, data_array in ds.data_vars.items():\n",
    "        if data_array.attrs[\"standard_name\"] in standard_names:\n",
    "            result.append(name)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_netcdf(selected: xr.Dataset):\n",
    "    \"\"\"Writing a NetCDF file\"\"\"\n",
    "    chunksizes = {}\n",
    "    for item in selected.data_vars:\n",
    "        chunks = selected[item].data.rechunk(block_size_limit=\"512KB\").chunks\n",
    "        chunksizes[item] = tuple(item[0] for item in chunks)\n",
    "\n",
    "    tmp = tempfile.NamedTemporaryFile(dir=TMPDIR).name\n",
    "    try:\n",
    "        encoding = dict((name, {\n",
    "            'zlib': True,\n",
    "            'complevel': 4,\n",
    "            'chunksizes': chunksizes[name]\n",
    "        }) for name in selected.data_vars)\n",
    "        selected.to_netcdf(tmp, mode=\"w\", encoding=encoding)\n",
    "    finally:\n",
    "        os.unlink(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TooBig(RuntimeError):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_benchmark(row: pd.DataFrame,\n",
    "                  ds: xr.Dataset,\n",
    "                  nbytes: np.ndarray,\n",
    "                  writer: Optional[Writers] = None,\n",
    "                  depth: str = \"depth\",\n",
    "                  lng: str = \"longitude\",\n",
    "                  lat: str = \"latitude\",\n",
    "                  time: str = \"time\",\n",
    "                  block_size_limit: str = \"256MB\") -> None:\n",
    "    \"\"\"Execute one benchmark\"\"\"\n",
    "    x0, x1 = row[\"minLongitude\"], row[\"maxLongitude\"]\n",
    "    y0, y1 = row[\"minLatitude\"], row[\"maxLatitude\"]\n",
    "    t0, t1 = row[\"minDate\"].to_datetime64(), row[\"maxDate\"].to_datetime64()\n",
    "    z0 = row[\"depth\"]\n",
    "\n",
    "    variables = varname_from_standard_name(ds, row[\"variables\"])\n",
    "\n",
    "    # Building the query\n",
    "    isel = {\n",
    "        lng: (ds[lng] >= x0) & (ds[lng] <= x1),\n",
    "        lat: (ds[lat] >= y0) & (ds[lat] <= y1),\n",
    "        time: (ds[time] >= t0) & (ds[time] <= t1)\n",
    "    }\n",
    "    if not np.isnan(z0):\n",
    "        isel[depth] = ds[depth] == z0\n",
    "\n",
    "    # Creation of the calculation graph performing the query\n",
    "    selected = ds.isel(isel)\n",
    "    selected = selected.drop_vars(set(ds.data_vars) - set(variables))\n",
    "    if not selected or not functools.reduce(operator.mul,\n",
    "                                            selected.dims.values()):\n",
    "        raise RuntimeError(f\"invalid query: {row}\")\n",
    "\n",
    "    # stores the size of the selected data\n",
    "    nbytes[0] = selected.nbytes\n",
    "    \n",
    "    # Reorganization of the selected zarr chunks\n",
    "    for item in selected.data_vars:\n",
    "        da = selected[item].data.rechunk(block_size_limit=block_size_limit)\n",
    "        chunk = dict(zip(selected[item].dims, da.chunks))\n",
    "        selected[item] = selected[item].chunk(chunk)\n",
    "        del selected[item].encoding['chunks']\n",
    "\n",
    "    if writer is Writers.NETCDF:\n",
    "        if selected.nbytes > 50 * 1000**3:\n",
    "            raise TooBig\n",
    "        # Write a temporary file in netCDF format.\n",
    "        write_netcdf(selected)\n",
    "    elif writer is Writers.ZARR:\n",
    "        # Write a temporary file in Zarr format.\n",
    "        selected.to_zarr(TMPZARR, mode=\"w\")\n",
    "    else:\n",
    "        # Measurement of data reading time\n",
    "        _ = selected.mean().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(\n",
    "        request: pd.DataFrame,\n",
    "        selected: pd.DataFrame,\n",
    "        dataset: str,\n",
    "        depth: str = \"depth\",\n",
    "        lng: str = \"longitude\",\n",
    "        lat: str = \"latitude\",\n",
    "        time: str = \"time\",\n",
    "        block_size_limit: str = \"256MB\",\n",
    "        write: bool = True\n",
    ") -> None:\n",
    "    \"\"\"Runs benchmarks on a given dataset\"\"\"\n",
    "    ds = xr.open_zarr(get_dataset_path(dataset), mask_and_scale=False)\n",
    "    nbytes = np.array([0], dtype=\"uint64\")\n",
    "    \n",
    "    for index, row in selected.iterrows():\n",
    "        if write and np.isnan(request.loc[index, 'netcdf']):\n",
    "            try:\n",
    "                timer = %timeit -r 1 -n 1 -o run_benchmark(row, ds, nbytes, Writers.NETCDF, depth, lng, lat, time, block_size_limit)\n",
    "                request.loc[index, 'netcdf'] = timer.average\n",
    "            except TooBig:\n",
    "                pass\n",
    "        if write and np.isnan(request.loc[index, 'zarr']):\n",
    "            try:\n",
    "                timer = %timeit -r 1 -n 1 -o run_benchmark(row, ds, nbytes, Writers.ZARR, depth, lng, lat, time, block_size_limit)\n",
    "                request.loc[index, 'zarr'] = timer.average\n",
    "            finally:\n",
    "                shutil.rmtree(TMPZARR, ignore_errors=True)\n",
    "        if np.isnan(request.loc[index, 'average']):\n",
    "            timer = %timeit -r 1 -n 1 -o run_benchmark(row, ds, nbytes, None, depth, lng, lat, time, block_size_limit)\n",
    "            request.loc[index, [\n",
    "                'average', 'best', 'stdev', 'worst', 'loops', 'repeat',\n",
    "                \"nbytes\"\n",
    "            ]] = (timer.average, timer.best, timer.stdev, timer.worst,\n",
    "                  timer.loops, timer.repeat, nbytes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(dataset: str,\n",
    "        cores: int,\n",
    "        request: pd.DataFrame,\n",
    "        depth: str = \"depth\",\n",
    "        lng: str = \"longitude\",\n",
    "        lat: str = \"latitude\",\n",
    "        time: str = \"time\",\n",
    "        block_size_limit: str = \"256MB\",\n",
    "        write: bool = True) -> None:\n",
    "    \"\"\"Run benchmarks for a given dataset\"\"\"\n",
    "    selected = request[request[\"Dataset\"] == dataset]\n",
    "    benchmark(request,\n",
    "              selected[selected[\"Cores\"] == cores],\n",
    "              dataset,\n",
    "              depth=depth,\n",
    "              lng=lng,\n",
    "              lat=lat,\n",
    "              time=time,\n",
    "              block_size_limit=block_size_limit,\n",
    "              write=write)\n",
    "    request.to_csv(RESULT, sep=\";\", date_format=\"%Y-%m-%d %H:%M:%S.000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = load_result(RESULT) if os.path.exists(RESULT) else load_request(REQUEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(\"global-analysis-forecast-phy-001-024\", 10, request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(\"global-analysis-forecast-phy-001-024\", 20, request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(\"global-analysis-forecast-phy-001-024\", 30, request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(\"global-analysis-forecast-phy-001-024\", 40, request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(\"global-analysis-forecast-phy-001-024\", 50, request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(\"global-analysis-forecast-phy-001-024-hourly-t-u-v-ssh\", 10, request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(\"LWQ100m\", 10, request, lng='lon', lat='lat', block_size_limit=\"32MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(\"LWQ100m\", 50, request, lng='lon', lat='lat', block_size_limit=\"100MB\", write=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = dask_jobqueue.PBSCluster(cores=1,\n",
    "                                   memory='16GB',\n",
    "                                   interface='ib0',\n",
    "                                   local_directory=\"$TMPDIR\",\n",
    "                                   walltime='12:00:00')\n",
    "cluster.scale(100)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = dask.distributed.Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(\"LWQ100m\", 100, request, lng='lon', lat='lat', block_size_limit=\"100MB\", write=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request.to_csv(RESULT, sep=\";\", date_format=\"%Y-%m-%d %H:%M:%S.000\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda",
   "language": "python",
   "name": "anaconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
