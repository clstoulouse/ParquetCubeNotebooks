{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, List, Optional\n",
    "import datetime\n",
    "import functools\n",
    "import operator\n",
    "import pyinterp.backends.xarray\n",
    "import os\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import dask.distributed\n",
    "import dask_jobqueue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = dask_jobqueue.PBSCluster(cores=1,\n",
    "                                   memory='2GB',\n",
    "                                   interface='ib0',\n",
    "                                   local_directory=\"$TMPDIR\",\n",
    "                                   walltime='12:00:00')\n",
    "cluster.scale(10)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = dask.distributed.Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root directory containing the datasets to be processed.\n",
    "ROOT = \"/home/ad/briolf/odatis/briolf/bigdata4science\"\n",
    "\n",
    "# File describing the queries to be executed .\n",
    "REQUEST = \"/home/ad/briolf/notebooks/bigdata4bigscience/requests.csv\"\n",
    "\n",
    "# File containing the results of the benchmarks\n",
    "RESULT = \"/home/ad/briolf/notebooks/bigdata4bigscience/interp.csv\"\n",
    "\n",
    "# Temporary directory\n",
    "TMPDIR = \"/work/ALT/odatis/briolf/tmp\"\n",
    "\n",
    "# Directory containing tracks data\n",
    "TRACKS = os.path.join(ROOT, \"tracks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCENARIO_1 = [\n",
    "    \"NorthSea.csv\", \n",
    "    \"Rotterdam12H.csv\", \n",
    "    \"Rotterdam15m.csv\", \n",
    "    \"Rotterdam1H.csv\", \n",
    "    \"Rotterdam1m.csv\", \n",
    "    \"RotterdamAll.csv\", \n",
    "]\n",
    "\n",
    "SCENARIO_2 = [\n",
    "    \"World10D.csv\", \n",
    "    \"World1.csv\", \n",
    "    \"World1D.csv\", \n",
    "    \"World20D.csv\", \n",
    "    \"World50D.csv\", \n",
    "    \"World5D.csv\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_request(path: str) -> pd.DataFrame:\n",
    "    \"\"\"Loading the track\"\"\"\n",
    "    result = pd.read_csv(path,\n",
    "                         sep=\";\",\n",
    "                         dtype={\n",
    "                             \"locDate\": \"str\",\n",
    "                             \"locTime\": \"str\",\n",
    "                             \"lon\": \"float64\",\n",
    "                             \"lat\": \"float64\",\n",
    "                         },\n",
    "                         parse_dates=False,\n",
    "                         usecols=[0, 1, 2, 3],\n",
    "                         engine='c')\n",
    "    dates = (\n",
    "        result['locDate'].apply(lambda x: x[-4:] + \"-\" + x[3:5] + \"-\" + x[:2])\n",
    "        + \"T\" + result['locTime']).values\n",
    "    result[\"datetime\"] = dates.astype(\"datetime64\")\n",
    "    result.drop(\"locDate\", axis=1, inplace=True)\n",
    "    result.drop(\"locTime\", axis=1, inplace=True)\n",
    "    result.sort_values(by=['datetime'], inplace=True, ignore_index=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_path(name: str) -> str:\n",
    "    \"\"\"Gets the dataset path\"\"\"\n",
    "    return os.path.join(ROOT, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def varname_from_standard_name(ds: xr.Dataset,\n",
    "                               standard_names: str) -> str:\n",
    "    \"\"\"Get variable names from standard names.\"\"\"\n",
    "    for name, data_array in ds.data_vars.items():\n",
    "        if data_array.attrs[\"standard_name\"] in standard_names:\n",
    "            return name\n",
    "    raise ValueError(f\"no such variable: {standard_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_(ds: xr.DataArray,\n",
    "                 df: pd.DataFrame,\n",
    "                 name: str,\n",
    "                 depth: Optional[float] = None) -> pd.Series:\n",
    "    dt = set(np.diff(ds[\"time\"]))\n",
    "    if len(dt) != 1:\n",
    "        raise RuntimeError(\"The deltaT between two grids is not constant.\")\n",
    "    dt = dt.pop()\n",
    "\n",
    "    dx = np.diff(ds[\"longitude\"]).max()\n",
    "    dy = np.diff(ds[\"latitude\"]).max()\n",
    "\n",
    "    x0 = df.lon.min() - dx\n",
    "    x1 = df.lon.max() + dx\n",
    "\n",
    "    y0 = df.lat.min() - dy\n",
    "    y1 = df.lat.max() + dy\n",
    "    \n",
    "    t0 = df.datetime.min()\n",
    "    t1 = df.datetime.max()\n",
    "    \n",
    "    it = ds.sel({'time': t0}, method='nearest').time.data\n",
    "    t0 = it - dt if it > t0 else t0\n",
    "    \n",
    "    it = ds.sel({'time': t1}, method='nearest').time.data\n",
    "    t1 = it + dt if it < t1 else t1\n",
    "\n",
    "    isel = {\n",
    "        \"longitude\": (ds[\"longitude\"] >= x0) & (ds[\"longitude\"] <= x1),\n",
    "        \"latitude\": (ds[\"latitude\"] >= y0) & (ds[\"latitude\"] <= y1),\n",
    "        \"time\": (ds[\"time\"] >= t0) & (ds[\"time\"] <= t1)\n",
    "    }\n",
    "\n",
    "    if depth is not None:\n",
    "        isel[\"depth\"] = ds[\"depth\"] == depth\n",
    "\n",
    "    # Creation of the calculation graph performing the query\n",
    "    selected = ds.isel(isel)\n",
    "    if not functools.reduce(operator.mul, selected.shape):\n",
    "        raise RuntimeError(f\"invalid query: {t0}, {t1}\")\n",
    "    if depth is not None:\n",
    "        selected = selected.squeeze(\"depth\")\n",
    "\n",
    "    selected = selected.compute()\n",
    "    interpolator = pyinterp.backends.xarray.RegularGridInterpolator(selected)\n",
    "    data = interpolator(dict(longitude=df.lon,\n",
    "                             latitude=df.lat,\n",
    "                             time=df.datetime.values),\n",
    "                        method=\"inverse_distance_weighting\",\n",
    "                        bounds_error=False,\n",
    "                        num_threads=0)\n",
    "    return pd.Series(data, df.index, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate(dataset: str,\n",
    "                df: pd.DataFrame,\n",
    "                name: str,\n",
    "                depth: Optional[float] = None) -> None:\n",
    "    \"\"\"Runs benchmarks on a given dataset\"\"\"\n",
    "    ds = xr.open_zarr(get_dataset_path(dataset))\n",
    "    period_start = df.groupby(df.datetime.dt.date).count().index\n",
    "\n",
    "    # fill variable to interpolate\n",
    "    df[name] = float(\"nan\")\n",
    "\n",
    "    # Calculates the period required to interpolate the data from the provided\n",
    "    # time series\n",
    "    end = None\n",
    "    periods = []\n",
    "    for start, end in zip(period_start, period_start[1:]):\n",
    "        start = pd.Timestamp(start)\n",
    "        end = pd.Timestamp(end)\n",
    "        periods.append([start, end])\n",
    "    if end is None:\n",
    "        end = pd.Timestamp(period_start[0])\n",
    "\n",
    "    # As the last date is excluded from the interval, a second is added to\n",
    "    # include it in the processing.\n",
    "    periods.append([end, df.datetime.iloc[-1] + datetime.timedelta(seconds=1)])\n",
    "\n",
    "    # Finally, the data on the different periods identified are interpolated.\n",
    "    futures = []\n",
    "    for start, end in periods:\n",
    "        varname = varname_from_standard_name(ds, name)\n",
    "        df_ = client.scatter(df[(df.datetime > start)\n",
    "                                & (df.datetime < end)])\n",
    "        futures.append(\n",
    "            client.submit(interpolate_, ds[varname], df_, name, depth))\n",
    "        #result = interpolate_(ds[varname], df[(df.datetime > start) & (df.datetime < end)], name, depth)\n",
    "    for item in dask.distributed.as_completed(futures):\n",
    "        series = item.result()\n",
    "        df.loc[series.index, series.name] = series.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=[\n",
    "    \"csv\", \"dataset\", \"variable\", \"average\", \"best\", \"worst\", \"stdev\", \"loop\",\n",
    "    \"repeat\", \"cores\"\n",
    "])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CASES_1 = {\n",
    "    \"global-analysis-forecast-phy-001-024\":\n",
    "    [(\"sea_surface_height_above_geoid\", None)],\n",
    "}\n",
    "\n",
    "CASES_2 = {\n",
    "    \"global-analysis-forecast-phy-001-024\": [\n",
    "        (\"sea_surface_height_above_geoid\", None),\n",
    "        (\"ocean_mixed_layer_thickness_defined_by_sigma_theta\", None),\n",
    "        (\"sea_water_potential_temperature\", 5.078224),\n",
    "        (\"eastward_sea_water_velocity\", 5.078224),\n",
    "        (\"northward_sea_water_velocity\", 5.078224),\n",
    "        (\"sea_water_salinity\", 5.078224),\n",
    "        (\"sea_water_potential_temperature_at_sea_floor\", None),\n",
    "        (\"sea_surface_height_above_geoid\", None),\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for csv in SCENARIO_1:\n",
    "    track = load_request(os.path.join(TRACKS, csv))\n",
    "    for dataset, cases in CASES_1.items():\n",
    "        for name, depth in cases:\n",
    "            timer = %timeit -r 3 -n 1 -o interpolate(dataset, track, name, depth=depth)\n",
    "            results = results.append(\n",
    "                pd.DataFrame(data=[[\n",
    "                    csv, dataset, name, timer.average, timer.best, timer.worst,\n",
    "                    timer.stdev, 3, 1, 10\n",
    "                ]],\n",
    "                             columns=[\n",
    "                                 \"csv\", \"dataset\", \"variable\", \"average\",\n",
    "                                 \"best\", \"worst\", \"stdev\", \"loop\", \"repeat\", \"cores\"\n",
    "                             ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for csv in SCENARIO_2:\n",
    "    track = load_request(os.path.join(TRACKS, csv))\n",
    "    for dataset, cases in CASES_2.items():\n",
    "        for name, depth in cases:\n",
    "            timer = %timeit -r 3 -n 1 -o interpolate(dataset, track, name, depth=depth)\n",
    "            results = results.append(\n",
    "                pd.DataFrame(data=[[\n",
    "                    csv, dataset, name, timer.average, timer.best, timer.worst,\n",
    "                    timer.stdev, 3, 1, 10\n",
    "                ]],\n",
    "                             columns=[\n",
    "                                 \"csv\", \"dataset\", \"variable\", \"average\",\n",
    "                                 \"best\", \"worst\", \"stdev\", \"loop\", \"repeat\", \"cores\"\n",
    "                             ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = \"World50D.csv\"\n",
    "track = load_request(os.path.join(TRACKS, csv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset, cases in CASES_2.items():\n",
    "    for name, depth in cases:\n",
    "        timer = %timeit -r 3 -n 1 -o interpolate(dataset, track, name, depth=depth)\n",
    "        results = results.append(\n",
    "            pd.DataFrame(data=[[\n",
    "                csv, dataset, name, timer.average, timer.best, timer.worst,\n",
    "                timer.stdev, 3, 1, 20\n",
    "            ]],\n",
    "                         columns=[\n",
    "                             \"csv\", \"dataset\", \"variable\", \"average\",\n",
    "                             \"best\", \"worst\", \"stdev\", \"loop\", \"repeat\", \"cores\"\n",
    "                         ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset, cases in CASES_2.items():\n",
    "    for name, depth in cases:\n",
    "        timer = %timeit -r 3 -n 1 -o interpolate(dataset, track, name, depth=depth)\n",
    "        results = results.append(\n",
    "            pd.DataFrame(data=[[\n",
    "                csv, dataset, name, timer.average, timer.best, timer.worst,\n",
    "                timer.stdev, 3, 1, 40\n",
    "            ]],\n",
    "                         columns=[\n",
    "                             \"csv\", \"dataset\", \"variable\", \"average\",\n",
    "                             \"best\", \"worst\", \"stdev\", \"loop\", \"repeat\", \"cores\"\n",
    "                         ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(RESULT, sep=\";\", date_format=\"%Y-%m-%d %H:%M:%S.000\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda",
   "language": "python",
   "name": "anaconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
